{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Bedrock unified use-case notebook (config + modular tests)\n\nThis notebook is refactored to keep **what belongs in YAML** vs **what belongs in notebook code** clearly separated.\n\n## YAML should store (stable config)\n\n- Auth/env var names\n- Endpoint/provider config\n- Model catalog (`models`)\n- Use-case registry (`use_cases`)\n- Quality levels and fallback chains (`presets` under each use case)\n\n## Notebook should store (test-time inputs)\n\n- Prompt text\n- Image/video file paths\n- Per-run overrides for generation/analysis\n- A/B test combinations\n\nSo prompts are now fully in notebook test functions, not in YAML.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# 0) Load config + auth\n# =========================\nimport os\nfrom pathlib import Path\n\ntry:\n    import yaml\nexcept ImportError as e:\n    raise ImportError(\"Missing dependency: pyyaml. Run: pip install pyyaml\") from e\n\nCONFIG_ENV = \"BEDROCK_HARNESS_CONFIG\"\nCONFIG_PATH = os.environ.get(CONFIG_ENV, \"bedrock_harness.yaml\")\ncfg_path = Path(CONFIG_PATH).expanduser().resolve()\nCFG_BASE_DIR = cfg_path.parent\n\nif not cfg_path.exists():\n    raise FileNotFoundError(\n        f\"Missing config file: {cfg_path}\\n\"\n        \"Put 'bedrock_harness.yaml' next to this notebook, or set:\\n\"\n        f\"  export {CONFIG_ENV}=/path/to/bedrock_harness.yaml\"\n    )\n\ncfg = yaml.safe_load(cfg_path.read_text(encoding=\"utf-8\")) or {}\n\nauth_cfg = cfg.get(\"auth\", {}) or {}\nTOKEN_ENV = auth_cfg.get(\"token_env\", \"AWS_BEARER_TOKEN_BEDROCK\")\nREGION_ENV = auth_cfg.get(\"region_env\", \"AWS_REGION\")\nDEFAULT_REGION = auth_cfg.get(\"default_region\", \"us-east-1\")\n\ntoken = (os.environ.get(TOKEN_ENV) or \"\").strip()\nif not token or len(token) < 20:\n    raise RuntimeError(\n        f\"Missing env var {TOKEN_ENV}.\\n\\n\"\n        \"Export it before launching VS Code / Jupyter, e.g.:\\n\"\n        f\"  export {TOKEN_ENV}='ABSK...'\\n\"\n        f\"  export {REGION_ENV}='us-east-1'\"\n    )\n\nAWS_REGION = (os.environ.get(REGION_ENV) or DEFAULT_REGION).strip()\nos.environ[REGION_ENV] = AWS_REGION\n\nuse_cases = cfg.get(\"use_cases\", {}) or {}\nif not use_cases:\n    raise ValueError(\"YAML missing `use_cases` section\")\n\nprint(\"Loaded config:\", str(cfg_path))\nprint(\"Region:\", AWS_REGION)\nprint(\"Token env:\", TOKEN_ENV, \"=\", \"set\" if bool(os.environ.get(TOKEN_ENV)) else \"missing\")\nprint(\"Use cases:\")\nfor uc_name, uc in use_cases.items():\n    presets = sorted((uc.get(\"presets\", {}) or {}).keys())\n    default_preset = uc.get(\"default_preset\")\n    adapter = uc.get(\"adapter\")\n    print(f\"  - {uc_name}: adapter={adapter}, default={default_preset}, presets={presets}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# 1) Unified modular harness\n# =========================\nfrom __future__ import annotations\n\nimport base64\nimport json\nimport mimetypes\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport requests\n\n\ndef _normalize_usage(provider: str, resp_json: Dict[str, Any]) -> Dict[str, Optional[int]]:\n    u = (resp_json or {}).get(\"usage\") or {}\n    inp = out = total = None\n\n    if provider == \"anthropic\":\n        inp = u.get(\"input_tokens\")\n        out = u.get(\"output_tokens\")\n        total = u.get(\"total_tokens\")\n\n    elif provider in {\"nova\", \"titan_image\"}:\n        inp = u.get(\"inputTokens\", u.get(\"input_tokens\"))\n        out = u.get(\"outputTokens\", u.get(\"output_tokens\"))\n        total = u.get(\"totalTokens\", u.get(\"total_tokens\"))\n\n    elif provider == \"openai_compat\":\n        inp = u.get(\"prompt_tokens\", u.get(\"input_tokens\"))\n        out = u.get(\"completion_tokens\", u.get(\"output_tokens\"))\n        total = u.get(\"total_tokens\")\n\n    if total is None and (inp is not None or out is not None):\n        total = (inp or 0) + (out or 0)\n\n    return {\n        \"input_tokens\": int(inp) if inp is not None else None,\n        \"output_tokens\": int(out) if out is not None else None,\n        \"total_tokens\": int(total) if total is not None else None,\n    }\n\n\n@dataclass(frozen=True)\nclass CandidateSpec:\n    use_case: str\n    adapter: str\n    media_kind: Optional[str]\n    preset: str\n    model_ref: str\n    provider: str\n    model_id: str\n    params: Dict[str, Any]\n\n\nclass UnifiedBedrockHarness:\n    def __init__(self, cfg: Dict[str, Any], config_dir: Path):\n        self.cfg = cfg or {}\n        self.config_dir = config_dir\n\n        auth = self.cfg.get(\"auth\", {}) or {}\n        self.token_env = auth.get(\"token_env\", \"AWS_BEARER_TOKEN_BEDROCK\")\n        self.region_env = auth.get(\"region_env\", \"AWS_REGION\")\n        self.default_region = auth.get(\"default_region\", \"us-east-1\")\n\n        http = self.cfg.get(\"http\", {}) or {}\n        self.timeout_seconds = int(http.get(\"timeout_seconds\", 120))\n\n        defaults = self.cfg.get(\"defaults\", {}) or {}\n        self.debug_fallback_default = bool(defaults.get(\"debug_fallback\", True))\n\n        endpoints = self.cfg.get(\"endpoints\", {}) or {}\n        self.runtime_base_tmpl = endpoints.get(\n            \"bedrock_runtime_base\",\n            \"https://bedrock-runtime.{region}.amazonaws.com\",\n        )\n\n        providers = self.cfg.get(\"providers\", {}) or {}\n        self.anthropic_version = ((providers.get(\"anthropic\", {}) or {}).get(\"anthropic_version\")) or \"bedrock-2023-05-31\"\n\n        openai_cfg = providers.get(\"openai_compat\", {}) or {}\n        self.openai_chat_path = openai_cfg.get(\"chat_completions_path\", \"/openai/v1/chat/completions\")\n        self.openai_system_role = openai_cfg.get(\"system_role\", \"developer\")\n        self.openai_max_tokens_param = openai_cfg.get(\"max_tokens_param\", \"max_completion_tokens\")\n\n        self.models = self.cfg.get(\"models\", {}) or {}\n        self.use_cases = self.cfg.get(\"use_cases\", {}) or {}\n\n        mm_cfg = self.cfg.get(\"multimodal\", {}) or {}\n        output_dir = mm_cfg.get(\"output_dir\", \"./outputs\")\n        self.output_dir = self.resolve_path(output_dir)\n\n        self._session = requests.Session()\n\n    # ---------- env ----------\n    def region(self) -> str:\n        return (os.environ.get(self.region_env) or self.default_region).strip()\n\n    def bearer_token(self) -> str:\n        tok = (os.environ.get(self.token_env) or \"\").strip()\n        if not tok or len(tok) < 20:\n            raise RuntimeError(f\"Missing env var {self.token_env}.\")\n        return tok\n\n    def runtime_base(self) -> str:\n        return self.runtime_base_tmpl.format(region=self.region()).rstrip(\"/\")\n\n    def headers_json(self) -> Dict[str, str]:\n        return {\n            \"Authorization\": f\"Bearer {self.bearer_token()}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    # ---------- config ----------\n    def resolve_path(self, p: str) -> Path:\n        path = Path(p).expanduser()\n        if path.is_absolute():\n            return path.resolve()\n        return (self.config_dir / path).resolve()\n\n    def list_use_cases(self) -> List[str]:\n        return sorted(self.use_cases.keys())\n\n    def list_presets(self, use_case: str) -> List[str]:\n        uc = self.use_cases.get(use_case) or {}\n        return sorted((uc.get(\"presets\", {}) or {}).keys())\n\n    def _resolve_model(self, model_ref: str) -> Tuple[str, str]:\n        if model_ref not in self.models:\n            raise ValueError(f\"Unknown model_ref '{model_ref}'. Available: {sorted(self.models.keys())}\")\n        m = self.models[model_ref] or {}\n        provider = m.get(\"provider\")\n        model_id = m.get(\"model_id\")\n        if not provider or not model_id:\n            raise ValueError(f\"Invalid model config for '{model_ref}': provider/model_id required\")\n        return provider, model_id\n\n    def build_candidate_specs(self, use_case: str, preset: Optional[str] = None) -> List[CandidateSpec]:\n        uc = self.use_cases.get(use_case)\n        if not uc:\n            raise ValueError(f\"Unknown use_case '{use_case}'. Available: {self.list_use_cases()}\")\n\n        adapter = uc.get(\"adapter\")\n        media_kind = uc.get(\"media_kind\")\n        default_preset = uc.get(\"default_preset\")\n        chosen_preset = preset or default_preset\n\n        presets = uc.get(\"presets\", {}) or {}\n        if chosen_preset not in presets:\n            raise ValueError(\n                f\"Unknown preset '{chosen_preset}' for use_case '{use_case}'. \"\n                f\"Available: {sorted(presets.keys())}\"\n            )\n\n        out: List[CandidateSpec] = []\n        for item in presets[chosen_preset]:\n            model_ref = item[\"model_ref\"]\n            provider, model_id = self._resolve_model(model_ref)\n            params = dict(item)\n            params.pop(\"model_ref\", None)\n            out.append(\n                CandidateSpec(\n                    use_case=use_case,\n                    adapter=adapter,\n                    media_kind=media_kind,\n                    preset=chosen_preset,\n                    model_ref=model_ref,\n                    provider=provider,\n                    model_id=model_id,\n                    params=params,\n                )\n            )\n        return out\n\n    # ---------- internal helpers ----------\n    def _effective_temperature(self, payload: Dict[str, Any], params: Dict[str, Any]) -> Optional[float]:\n        if \"temperature\" in payload and payload.get(\"temperature\") is not None:\n            return float(payload.get(\"temperature\"))\n        if \"temperature\" in params and params.get(\"temperature\") is not None:\n            return float(params.get(\"temperature\"))\n        return None\n\n    def _effective_max_tokens(self, payload: Dict[str, Any], params: Dict[str, Any], fallback: int = 1024) -> int:\n        if \"max_output_tokens\" in payload and payload.get(\"max_output_tokens\") is not None:\n            return int(payload.get(\"max_output_tokens\"))\n        if \"max_output_tokens\" in params and params.get(\"max_output_tokens\") is not None:\n            return int(params.get(\"max_output_tokens\"))\n        return int(fallback)\n\n    def _anthropic_thinking(self, params: Dict[str, Any], max_tokens: int) -> Optional[Dict[str, Any]]:\n        anth = params.get(\"anthropic\", {}) or {}\n        thinking = (anth.get(\"thinking\", {}) or {})\n        enabled = bool(thinking.get(\"enabled\", False))\n        if not enabled:\n            return None\n\n        out: Dict[str, Any] = {\"type\": \"enabled\"}\n        if \"budget_tokens\" in thinking and thinking.get(\"budget_tokens\") is not None:\n            budget = int(thinking.get(\"budget_tokens\"))\n            out[\"budget_tokens\"] = max(1, min(max_tokens - 1, budget))\n        return out\n\n    def _save_base64_file(self, use_case: str, scenario_tag: str, b64_text: str, ext: str) -> str:\n        uc_cfg = self.use_cases.get(use_case, {}) or {}\n        output_cfg = uc_cfg.get(\"output\", {}) or {}\n        subdir = output_cfg.get(\"save_subdir\", use_case)\n\n        out_dir = (self.output_dir / subdir).resolve()\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        ts = int(time.time())\n        out_path = out_dir / f\"{scenario_tag}_{ts}.{ext.lstrip('.')}\"\n        out_path.write_bytes(base64.b64decode(b64_text))\n        return str(out_path)\n\n    # ---------- adapter: conversation ----------\n    def _invoke_conversation(self, spec: CandidateSpec, payload: Dict[str, Any]) -> Dict[str, Any]:\n        system_prompt = str(payload.get(\"system_prompt\", \"\") or \"\").strip()\n        user_prompt = str(payload.get(\"user_prompt\", \"\") or \"\").strip()\n        if not user_prompt:\n            raise ValueError(\"conversation requires payload.user_prompt\")\n\n        if spec.provider == \"anthropic\":\n            return self._invoke_anthropic_text(spec, system_prompt, user_prompt, payload)\n        if spec.provider == \"nova\":\n            return self._invoke_nova_text(spec, system_prompt, user_prompt, payload)\n        if spec.provider == \"openai_compat\":\n            return self._invoke_openai_text(spec, system_prompt, user_prompt, payload)\n\n        raise ValueError(f\"Unsupported provider '{spec.provider}' for conversation\")\n\n    def _invoke_anthropic_text(self, spec: CandidateSpec, system_prompt: str, user_prompt: str, payload: Dict[str, Any]) -> Dict[str, Any]:\n        max_tokens = self._effective_max_tokens(payload, spec.params, fallback=1024)\n        temperature = self._effective_temperature(payload, spec.params)\n        thinking = self._anthropic_thinking(spec.params, max_tokens)\n\n        req: Dict[str, Any] = {\n            \"anthropic_version\": self.anthropic_version,\n            \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_prompt}]}],\n            \"max_tokens\": max_tokens,\n        }\n        if system_prompt:\n            req[\"system\"] = system_prompt\n        if temperature is not None:\n            req[\"temperature\"] = 1.0 if thinking else temperature\n        if thinking is not None:\n            req[\"thinking\"] = thinking\n\n        url = f\"{self.runtime_base()}/model/{spec.model_id}/invoke\"\n        r = self._session.post(url, headers=self.headers_json(), data=json.dumps(req), timeout=self.timeout_seconds)\n        if r.status_code != 200:\n            raise RuntimeError(f\"Anthropic invoke failed {r.status_code}: {r.text[:1200]}\")\n\n        resp = r.json()\n        blocks = resp.get(\"content\", []) or []\n        text = \"\".join([b.get(\"text\", \"\") for b in blocks if isinstance(b, dict) and b.get(\"type\") == \"text\"]).strip()\n        usage = _normalize_usage(\"anthropic\", resp)\n\n        return {\"response\": text, \"usage\": usage, \"raw\": resp, \"saved_files\": [], \"job_id\": None}\n\n    def _invoke_nova_text(self, spec: CandidateSpec, system_prompt: str, user_prompt: str, payload: Dict[str, Any]) -> Dict[str, Any]:\n        max_tokens = self._effective_max_tokens(payload, spec.params, fallback=1024)\n        temperature = self._effective_temperature(payload, spec.params)\n\n        req: Dict[str, Any] = {\n            \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": user_prompt}]}],\n            \"inferenceConfig\": {\"maxTokens\": max_tokens},\n        }\n        if system_prompt:\n            req[\"system\"] = [{\"text\": system_prompt}]\n        if temperature is not None:\n            req[\"inferenceConfig\"][\"temperature\"] = temperature\n\n        url = f\"{self.runtime_base()}/model/{spec.model_id}/invoke\"\n        r = self._session.post(url, headers=self.headers_json(), data=json.dumps(req), timeout=self.timeout_seconds)\n        if r.status_code != 200:\n            raise RuntimeError(f\"Nova invoke failed {r.status_code}: {r.text[:1200]}\")\n\n        resp = r.json()\n        content = (((resp.get(\"output\") or {}).get(\"message\") or {}).get(\"content\")) or []\n        text = \"\".join([c.get(\"text\", \"\") for c in content if isinstance(c, dict) and \"text\" in c]).strip()\n        usage = _normalize_usage(\"nova\", resp)\n\n        return {\"response\": text, \"usage\": usage, \"raw\": resp, \"saved_files\": [], \"job_id\": None}\n\n    def _invoke_openai_text(self, spec: CandidateSpec, system_prompt: str, user_prompt: str, payload: Dict[str, Any]) -> Dict[str, Any]:\n        max_tokens = self._effective_max_tokens(payload, spec.params, fallback=1024)\n        temperature = self._effective_temperature(payload, spec.params)\n\n        messages: List[Dict[str, Any]] = []\n        if system_prompt:\n            messages.append({\"role\": self.openai_system_role, \"content\": system_prompt})\n        messages.append({\"role\": \"user\", \"content\": user_prompt})\n\n        req: Dict[str, Any] = {\n            \"model\": spec.model_id,\n            \"messages\": messages,\n            \"stream\": False,\n            self.openai_max_tokens_param: max_tokens,\n        }\n        if temperature is not None:\n            req[\"temperature\"] = temperature\n\n        url = f\"{self.runtime_base()}{self.openai_chat_path}\"\n        r = self._session.post(url, headers=self.headers_json(), data=json.dumps(req), timeout=self.timeout_seconds)\n        if r.status_code != 200:\n            raise RuntimeError(f\"OpenAI-compat invoke failed {r.status_code}: {r.text[:1200]}\")\n\n        resp = r.json()\n        text = (resp.get(\"choices\", [{}])[0].get(\"message\", {}) or {}).get(\"content\", \"\") or \"\"\n        usage = _normalize_usage(\"openai_compat\", resp)\n\n        return {\"response\": text, \"usage\": usage, \"raw\": resp, \"saved_files\": [], \"job_id\": None}\n\n    # ---------- adapter: understanding ----------\n    def _invoke_understanding(self, spec: CandidateSpec, payload: Dict[str, Any]) -> Dict[str, Any]:\n        if spec.provider != \"anthropic\":\n            raise ValueError(\n                f\"Use case '{spec.use_case}' currently supports anthropic provider only for understanding; got {spec.provider}\"\n            )\n\n        media_path_val = payload.get(\"media_path\")\n        if not media_path_val:\n            raise ValueError(f\"{spec.use_case} requires payload.media_path\")\n\n        media_path = self.resolve_path(str(media_path_val))\n        if not media_path.exists():\n            raise FileNotFoundError(f\"Media file not found: {media_path}\")\n\n        media_kind = spec.media_kind or payload.get(\"media_kind\") or \"image\"\n        system_prompt = str(payload.get(\"system_prompt\", \"\") or \"\").strip()\n        user_prompt = str(payload.get(\"user_prompt\", \"\") or \"\").strip()\n\n        media_type = payload.get(\"media_type\")\n        if not media_type:\n            guessed, _ = mimetypes.guess_type(str(media_path))\n            if guessed:\n                media_type = guessed\n            else:\n                media_type = \"image/jpeg\" if media_kind == \"image\" else \"video/mp4\"\n\n        b64 = base64.b64encode(media_path.read_bytes()).decode(\"ascii\")\n\n        max_tokens = self._effective_max_tokens(payload, spec.params, fallback=1400)\n        temperature = self._effective_temperature(payload, spec.params)\n        thinking = self._anthropic_thinking(spec.params, max_tokens)\n\n        content: List[Dict[str, Any]] = [\n            {\n                \"type\": media_kind,\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": media_type,\n                    \"data\": b64,\n                },\n            }\n        ]\n        if user_prompt:\n            content.append({\"type\": \"text\", \"text\": user_prompt})\n\n        req: Dict[str, Any] = {\n            \"anthropic_version\": self.anthropic_version,\n            \"messages\": [{\"role\": \"user\", \"content\": content}],\n            \"max_tokens\": max_tokens,\n        }\n        if system_prompt:\n            req[\"system\"] = system_prompt\n        if temperature is not None:\n            req[\"temperature\"] = 1.0 if thinking else temperature\n        if thinking is not None:\n            req[\"thinking\"] = thinking\n\n        url = f\"{self.runtime_base()}/model/{spec.model_id}/invoke\"\n        r = self._session.post(url, headers=self.headers_json(), data=json.dumps(req), timeout=self.timeout_seconds)\n        if r.status_code != 200:\n            raise RuntimeError(f\"Understanding invoke failed {r.status_code}: {r.text[:1200]}\")\n\n        resp = r.json()\n        blocks = resp.get(\"content\", []) or []\n        text = \"\".join([b.get(\"text\", \"\") for b in blocks if isinstance(b, dict) and b.get(\"type\") == \"text\"]).strip()\n        usage = _normalize_usage(\"anthropic\", resp)\n\n        return {\"response\": text, \"usage\": usage, \"raw\": resp, \"saved_files\": [], \"job_id\": None}\n\n    # ---------- adapter: image_generation ----------\n    def _invoke_image_generation(self, spec: CandidateSpec, payload: Dict[str, Any]) -> Dict[str, Any]:\n        if spec.provider not in {\"nova\", \"titan_image\"}:\n            raise ValueError(f\"Unsupported provider '{spec.provider}' for image_generation\")\n\n        prompt = str(payload.get(\"prompt\", \"\") or \"\").strip()\n        if not prompt:\n            raise ValueError(\"image_generation requires payload.prompt\")\n\n        negative_prompt = str(payload.get(\"negative_prompt\", \"\") or \"\").strip()\n\n        width = int(payload.get(\"width\") or spec.params.get(\"width\") or 1024)\n        height = int(payload.get(\"height\") or spec.params.get(\"height\") or 1024)\n        number_of_images = int(payload.get(\"number_of_images\") or spec.params.get(\"number_of_images\") or 1)\n        cfg_scale = float(payload.get(\"cfg_scale\") or spec.params.get(\"cfg_scale\") or 8.0)\n        seed = payload.get(\"seed\", spec.params.get(\"seed\"))\n\n        req: Dict[str, Any] = {\n            \"taskType\": \"TEXT_IMAGE\",\n            \"textToImageParams\": {\"text\": prompt},\n            \"imageGenerationConfig\": {\n                \"numberOfImages\": number_of_images,\n                \"width\": width,\n                \"height\": height,\n                \"cfgScale\": cfg_scale,\n            },\n        }\n        if negative_prompt:\n            req[\"textToImageParams\"][\"negativeText\"] = negative_prompt\n        if seed is not None:\n            req[\"imageGenerationConfig\"][\"seed\"] = int(seed)\n\n        url = f\"{self.runtime_base()}/model/{spec.model_id}/invoke\"\n        r = self._session.post(url, headers=self.headers_json(), data=json.dumps(req), timeout=self.timeout_seconds)\n        if r.status_code != 200:\n            raise RuntimeError(f\"Image generation invoke failed {r.status_code}: {r.text[:1200]}\")\n\n        resp = r.json()\n\n        images: List[str] = []\n        if isinstance(resp.get(\"images\"), list):\n            for it in resp.get(\"images\"):\n                if isinstance(it, str):\n                    images.append(it)\n\n        if isinstance(resp.get(\"artifacts\"), list):\n            for it in resp.get(\"artifacts\"):\n                if isinstance(it, dict):\n                    b64 = it.get(\"base64\") or it.get(\"data\") or it.get(\"image\")\n                    if isinstance(b64, str):\n                        images.append(b64)\n\n        saved_files: List[str] = []\n        uc_output = (self.use_cases.get(spec.use_case, {}) or {}).get(\"output\", {}) or {}\n        ext = uc_output.get(\"file_ext\", \"png\")\n        for idx, b64_img in enumerate(images, start=1):\n            saved_files.append(self._save_base64_file(spec.use_case, f\"{spec.preset}_{idx:02d}\", b64_img, ext))\n\n        response_text = f\"Generated {len(saved_files)} image(s).\" if saved_files else \"No image artifacts found in response.\"\n        usage = _normalize_usage(spec.provider, resp)\n\n        return {\"response\": response_text, \"usage\": usage, \"raw\": resp, \"saved_files\": saved_files, \"job_id\": None}\n\n    # ---------- adapter: video_generation ----------\n    def _invoke_video_generation(self, spec: CandidateSpec, payload: Dict[str, Any]) -> Dict[str, Any]:\n        if spec.provider != \"nova\":\n            raise ValueError(f\"Unsupported provider '{spec.provider}' for video_generation\")\n\n        prompt = str(payload.get(\"prompt\", \"\") or \"\").strip()\n        if not prompt:\n            raise ValueError(\"video_generation requires payload.prompt\")\n\n        duration_seconds = int(payload.get(\"duration_seconds\") or spec.params.get(\"duration_seconds\") or 6)\n        fps = int(payload.get(\"fps\") or spec.params.get(\"fps\") or 24)\n        dimension = str(payload.get(\"dimension\") or spec.params.get(\"dimension\") or \"1280x720\")\n        seed = payload.get(\"seed\", spec.params.get(\"seed\"))\n\n        req: Dict[str, Any] = {\n            \"taskType\": \"TEXT_VIDEO\",\n            \"textToVideoParams\": {\"text\": prompt},\n            \"videoGenerationConfig\": {\n                \"durationSeconds\": duration_seconds,\n                \"fps\": fps,\n                \"dimension\": dimension,\n            },\n        }\n        if seed is not None:\n            req[\"videoGenerationConfig\"][\"seed\"] = int(seed)\n\n        url = f\"{self.runtime_base()}/model/{spec.model_id}/invoke\"\n        r = self._session.post(url, headers=self.headers_json(), data=json.dumps(req), timeout=self.timeout_seconds)\n        if r.status_code != 200:\n            raise RuntimeError(f\"Video generation invoke failed {r.status_code}: {r.text[:1200]}\")\n\n        resp = r.json()\n\n        video_blob = (\n            resp.get(\"video\")\n            or resp.get(\"videoBase64\")\n            or ((resp.get(\"output\") or {}).get(\"video\") if isinstance(resp.get(\"output\"), dict) else None)\n            or ((resp.get(\"result\") or {}).get(\"video\") if isinstance(resp.get(\"result\"), dict) else None)\n        )\n\n        job_id = (\n            resp.get(\"invocationArn\")\n            or resp.get(\"jobArn\")\n            or resp.get(\"jobId\")\n            or resp.get(\"id\")\n        )\n\n        saved_files: List[str] = []\n        if isinstance(video_blob, str) and len(video_blob) > 100:\n            uc_output = (self.use_cases.get(spec.use_case, {}) or {}).get(\"output\", {}) or {}\n            ext = uc_output.get(\"file_ext\", \"mp4\")\n            saved_files.append(self._save_base64_file(spec.use_case, spec.preset, video_blob, ext))\n\n        if saved_files:\n            response_text = f\"Generated {len(saved_files)} video artifact(s).\"\n        elif job_id:\n            response_text = f\"Video generation accepted. job_id={job_id}\"\n        else:\n            response_text = \"No video artifact/job id found in response.\"\n\n        usage = _normalize_usage(spec.provider, resp)\n        return {\"response\": response_text, \"usage\": usage, \"raw\": resp, \"saved_files\": saved_files, \"job_id\": str(job_id) if job_id else None}\n\n    # ---------- public API ----------\n    def run(\n        self,\n        use_case: str,\n        payload: Dict[str, Any],\n        preset: Optional[str] = None,\n        *,\n        return_record: bool = False,\n        label: str = \"\",\n        debug_fallback: Optional[bool] = None,\n    ) -> Union[str, Dict[str, Any]]:\n        if debug_fallback is None:\n            debug_fallback = self.debug_fallback_default\n\n        candidates = self.build_candidate_specs(use_case, preset)\n        t0 = time.time()\n        last_err: Optional[Exception] = None\n\n        for idx, spec in enumerate(candidates, start=1):\n            try:\n                if spec.adapter == \"conversation\":\n                    out = self._invoke_conversation(spec, payload)\n                elif spec.adapter == \"understanding\":\n                    out = self._invoke_understanding(spec, payload)\n                elif spec.adapter == \"image_generation\":\n                    out = self._invoke_image_generation(spec, payload)\n                elif spec.adapter == \"video_generation\":\n                    out = self._invoke_video_generation(spec, payload)\n                else:\n                    raise ValueError(f\"Unsupported adapter '{spec.adapter}' for use_case '{use_case}'\")\n\n                latency = round(time.time() - t0, 3)\n                if return_record:\n                    return {\n                        \"label\": label or spec.preset,\n                        \"use_case\": use_case,\n                        \"adapter\": spec.adapter,\n                        \"preset\": spec.preset,\n                        \"picked_index\": idx,\n                        \"model_ref\": spec.model_ref,\n                        \"provider\": spec.provider,\n                        \"model_id\": spec.model_id,\n                        \"params\": spec.params,\n                        \"latency_s\": latency,\n                        \"usage\": out.get(\"usage\"),\n                        \"response\": out.get(\"response\"),\n                        \"saved_files\": out.get(\"saved_files\") or [],\n                        \"job_id\": out.get(\"job_id\"),\n                        \"error\": None,\n                    }\n                return str(out.get(\"response\", \"\"))\n\n            except Exception as e:\n                last_err = e\n                if debug_fallback:\n                    print(\n                        f\"[fallback] use_case={use_case} preset={spec.preset} \"\n                        f\"model_ref={spec.model_ref} provider={spec.provider} failed: {type(e).__name__}: {e}\"\n                    )\n                continue\n\n        latency = round(time.time() - t0, 3)\n        err_text = f\"{type(last_err).__name__}: {last_err}\" if last_err else \"Unknown error\"\n        if return_record:\n            return {\n                \"label\": label or (preset or \"default\"),\n                \"use_case\": use_case,\n                \"adapter\": None,\n                \"preset\": preset,\n                \"picked_index\": None,\n                \"model_ref\": None,\n                \"provider\": None,\n                \"model_id\": None,\n                \"params\": None,\n                \"latency_s\": latency,\n                \"usage\": None,\n                \"response\": None,\n                \"saved_files\": [],\n                \"job_id\": None,\n                \"error\": err_text,\n            }\n        return f\"An error occurred: {err_text}\"\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# 2) Test modules (prompts and inputs live here)\n# =========================\nh = UnifiedBedrockHarness(cfg, config_dir=CFG_BASE_DIR)\n\n\ndef test_conversation(\n    system_prompt: str,\n    user_prompt: str,\n    *,\n    level: Optional[str] = None,\n    label: str = \"\",\n    debug_fallback: Optional[bool] = None,\n):\n    payload = {\n        \"system_prompt\": system_prompt,\n        \"user_prompt\": user_prompt,\n    }\n    return h.run(\n        \"conversation\",\n        payload,\n        preset=level,\n        return_record=True,\n        label=label or (level or \"conversation\"),\n        debug_fallback=debug_fallback,\n    )\n\n\ndef test_image_understanding(\n    image_path: str,\n    user_prompt: str,\n    *,\n    system_prompt: str = \"You are a visual analyst. Be precise and concise.\",\n    level: Optional[str] = None,\n    media_type: Optional[str] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    label: str = \"\",\n    debug_fallback: Optional[bool] = None,\n):\n    payload: Dict[str, Any] = {\n        \"media_path\": image_path,\n        \"user_prompt\": user_prompt,\n        \"system_prompt\": system_prompt,\n    }\n    if media_type:\n        payload[\"media_type\"] = media_type\n    if max_output_tokens is not None:\n        payload[\"max_output_tokens\"] = int(max_output_tokens)\n    if temperature is not None:\n        payload[\"temperature\"] = float(temperature)\n\n    return h.run(\n        \"image_understanding\",\n        payload,\n        preset=level,\n        return_record=True,\n        label=label or (level or \"image_understanding\"),\n        debug_fallback=debug_fallback,\n    )\n\n\ndef test_video_understanding(\n    video_path: str,\n    user_prompt: str,\n    *,\n    system_prompt: str = \"You are a video analyst. Summarize actions, timeline, and key events.\",\n    level: Optional[str] = None,\n    media_type: Optional[str] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    label: str = \"\",\n    debug_fallback: Optional[bool] = None,\n):\n    payload: Dict[str, Any] = {\n        \"media_path\": video_path,\n        \"user_prompt\": user_prompt,\n        \"system_prompt\": system_prompt,\n    }\n    if media_type:\n        payload[\"media_type\"] = media_type\n    if max_output_tokens is not None:\n        payload[\"max_output_tokens\"] = int(max_output_tokens)\n    if temperature is not None:\n        payload[\"temperature\"] = float(temperature)\n\n    return h.run(\n        \"video_understanding\",\n        payload,\n        preset=level,\n        return_record=True,\n        label=label or (level or \"video_understanding\"),\n        debug_fallback=debug_fallback,\n    )\n\n\ndef test_image_generation(\n    prompt: str,\n    *,\n    negative_prompt: str = \"\",\n    level: Optional[str] = None,\n    width: Optional[int] = None,\n    height: Optional[int] = None,\n    number_of_images: Optional[int] = None,\n    cfg_scale: Optional[float] = None,\n    seed: Optional[int] = None,\n    label: str = \"\",\n    debug_fallback: Optional[bool] = None,\n):\n    payload: Dict[str, Any] = {\n        \"prompt\": prompt,\n        \"negative_prompt\": negative_prompt,\n    }\n    if width is not None:\n        payload[\"width\"] = int(width)\n    if height is not None:\n        payload[\"height\"] = int(height)\n    if number_of_images is not None:\n        payload[\"number_of_images\"] = int(number_of_images)\n    if cfg_scale is not None:\n        payload[\"cfg_scale\"] = float(cfg_scale)\n    if seed is not None:\n        payload[\"seed\"] = int(seed)\n\n    return h.run(\n        \"image_generation\",\n        payload,\n        preset=level,\n        return_record=True,\n        label=label or (level or \"image_generation\"),\n        debug_fallback=debug_fallback,\n    )\n\n\ndef test_video_generation(\n    prompt: str,\n    *,\n    level: Optional[str] = None,\n    duration_seconds: Optional[int] = None,\n    fps: Optional[int] = None,\n    dimension: Optional[str] = None,\n    seed: Optional[int] = None,\n    label: str = \"\",\n    debug_fallback: Optional[bool] = None,\n):\n    payload: Dict[str, Any] = {\"prompt\": prompt}\n    if duration_seconds is not None:\n        payload[\"duration_seconds\"] = int(duration_seconds)\n    if fps is not None:\n        payload[\"fps\"] = int(fps)\n    if dimension is not None:\n        payload[\"dimension\"] = str(dimension)\n    if seed is not None:\n        payload[\"seed\"] = int(seed)\n\n    return h.run(\n        \"video_generation\",\n        payload,\n        preset=level,\n        return_record=True,\n        label=label or (level or \"video_generation\"),\n        debug_fallback=debug_fallback,\n    )\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# 3) Display helpers\n# =========================\nfrom IPython.display import Markdown, display\nimport html\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional\n\n\ndef display_records(*records: Dict[str, Any], title: Optional[str] = None, max_col_width_px: int = 520):\n    cols = list(records[0]) if (len(records) == 1 and isinstance(records[0], (list, tuple))) else list(records)\n\n    def esc(x: Any) -> str:\n        s = \"\" if x is None else str(x)\n        return html.escape(s).replace(\"\\n\", \"<br/>\")\n\n    def fmt_tokens(rec: Dict[str, Any]) -> str:\n        u = rec.get(\"usage\") or {}\n        if not isinstance(u, dict) or not u:\n            return \"\"\n        inp = u.get(\"input_tokens\")\n        out = u.get(\"output_tokens\")\n        tot = u.get(\"total_tokens\")\n        parts = []\n        if inp is not None:\n            parts.append(f\"in:{inp}\")\n        if out is not None:\n            parts.append(f\"out:{out}\")\n        if tot is not None:\n            parts.append(f\"total:{tot}\")\n        return \"tokens: \" + \" · \".join(parts) if parts else \"\"\n\n    def header_block(rec: Dict[str, Any]) -> str:\n        lines = [\n            f\"use_case: {rec.get('use_case')}\",\n            f\"preset: {rec.get('preset')}\",\n            f\"picked: #{rec.get('picked_index')}\",\n            f\"provider: {rec.get('provider')}\",\n            f\"model_ref: {rec.get('model_ref')}\",\n            f\"model_id: {rec.get('model_id')}\",\n            f\"latency: {rec.get('latency_s')}s\",\n        ]\n        if rec.get(\"job_id\"):\n            lines.append(f\"job_id: {rec.get('job_id')}\")\n        tok = fmt_tokens(rec)\n        if tok:\n            lines.append(tok)\n        if rec.get(\"error\"):\n            lines.append(f\"error: {rec.get('error')}\")\n\n        files = rec.get(\"saved_files\") or []\n        if files:\n            lines.append(\"saved_files:\")\n            for fp in files:\n                lines.append(f\"  - {fp}\")\n\n        return \"<br/>\".join(esc(x) for x in lines if x and x != \"#None\")\n\n    table = []\n    table.append(\"<div style='overflow-x:auto; padding:6px 2px;'>\")\n    table.append(\"<table style='border-collapse:collapse; width:max-content; min-width:100%;'>\")\n\n    if title:\n        table.append(\n            f\"<tr><th colspan='{len(cols)}' \"\n            \"style='text-align:left; font-size:16px; padding:10px; border:1px solid #ddd; background:#fafafa;'>\"\n            f\"{esc(title)}</th></tr>\"\n        )\n\n    table.append(\"<tr>\")\n    for i, rec in enumerate(cols):\n        label = rec.get(\"label\") or f\"Variant {i+1}\"\n        meta = header_block(rec)\n        table.append(\n            \"<th style='text-align:left; vertical-align:top; border:1px solid #ddd; padding:10px; \"\n            \"background:#f7f7f7; min-width:320px; \"\n            f\"max-width:{max_col_width_px}px;'>\"\n            f\"{esc(label)}<br/><span style='font-weight:normal;color:#666'>{meta}</span></th>\"\n        )\n    table.append(\"</tr>\")\n\n    table.append(\"<tr>\")\n    for rec in cols:\n        resp = rec.get(\"response\", \"\")\n        table.append(\n            \"<td style='vertical-align:top; border:1px solid #ddd; padding:10px; \"\n            f\"max-width:{max_col_width_px}px;'>\"\n            \"<div><b>Response</b></div>\"\n            f\"<div style='margin-top:6px; line-height:1.35;'>{esc(resp)}</div>\"\n            \"</td>\"\n        )\n    table.append(\"</tr>\")\n\n    table.append(\"</table></div>\")\n    display(Markdown(\"\".join(table)))\n\n\n# Optional: quickly check artifact files exist\n\ndef print_artifact_status(rec: Dict[str, Any]):\n    files = rec.get(\"saved_files\") or []\n    if not files:\n        return\n    for fp in files:\n        p = Path(fp)\n        print(\"artifact:\", p, \"exists=\" + str(p.exists()))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# 4) Example tests (prompts are in notebook, not YAML)\n# =========================\n\n# A) Conversation A/B\nSYSTEM = \"You are an AI engineering expert. Explain with practical examples.\"\nUSER = \"RAG、MCP、Skill 的区别是什么？请用中文给我一个工程实践角度的解释。\"\n\nresp_conv_default = test_conversation(SYSTEM, USER, level=\"analysis_default\", label=\"conv_default\")\nresp_conv_max = test_conversation(SYSTEM, USER, level=\"analysis_max\", label=\"conv_max\")\n\ndisplay_records(resp_conv_default, resp_conv_max, title=\"Conversation: analysis_default vs analysis_max\")\n\n# B) Image understanding (run only when file exists)\nimg_default = cfg.get(\"multimodal\", {}).get(\"default_image_path\", \"./samples/demo_image.jpg\")\nimg_abs = h.resolve_path(img_default)\nif img_abs.exists():\n    resp_img = test_image_understanding(\n        str(img_abs),\n        \"请提取图中关键元素，并给出场景推断。\",\n        level=\"vision_default\",\n        label=\"image_understanding_default\",\n    )\n    display_records(resp_img, title=\"Image Understanding\")\nelse:\n    print(f\"Skip image_understanding: file not found -> {img_abs}\")\n\n# C) Video understanding (run only when file exists)\nvideo_default = cfg.get(\"multimodal\", {}).get(\"default_video_path\", \"./samples/demo_video.mp4\")\nvideo_abs = h.resolve_path(video_default)\nif video_abs.exists():\n    resp_video = test_video_understanding(\n        str(video_abs),\n        \"请按时间顺序总结视频里的主要事件。\",\n        level=\"video_default\",\n        label=\"video_understanding_default\",\n    )\n    display_records(resp_video, title=\"Video Understanding\")\nelse:\n    print(f\"Skip video_understanding: file not found -> {video_abs}\")\n\n# D) Generation examples moved to separate code cells below.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# 5) Image Generation (separate chunk)\n# =========================\nRUN_IMAGE_GENERATION = False\n\nif RUN_IMAGE_GENERATION:\n    resp_img_gen = test_image_generation(\n        prompt=\"A modern electric concept car in a minimalist studio, cinematic lighting.\",\n        level=\"image_default\",\n        negative_prompt=\"blurry, text watermark\",\n        label=\"image_generation_default\",\n    )\n    display_records(resp_img_gen, title=\"Image Generation\")\n    print_artifact_status(resp_img_gen)\nelse:\n    print(\"Skip image generation. Set RUN_IMAGE_GENERATION=True to run.\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# 6) Video Generation (separate chunk)\n# =========================\nRUN_VIDEO_GENERATION = False\n\nif RUN_VIDEO_GENERATION:\n    resp_video_gen = test_video_generation(\n        prompt=\"A smooth drone shot above ocean cliffs at sunset.\",\n        level=\"video_default\",\n        label=\"video_generation_default\",\n    )\n    display_records(resp_video_gen, title=\"Video Generation\")\n    print_artifact_status(resp_video_gen)\nelse:\n    print(\"Skip video generation. Set RUN_VIDEO_GENERATION=True to run.\")\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}