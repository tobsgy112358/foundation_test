{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Bedrock multimodal test notebook (config-driven)\n\nThis notebook targets non-text scenarios:\n\n- Image generation\n- Video generation\n- Image understanding\n- Video understanding\n\nDesign principle:\n- Keep **all tunable parameters** in `bedrock_harness.yaml`.\n- Keep this notebook focused on loading config and executing scenarios.\n\n## One-time setup\n\n```bash\nexport AWS_BEARER_TOKEN_BEDROCK=\"ABSK...\"\nexport AWS_REGION=\"us-east-1\"   # optional\npip install requests pyyaml\n```\n\n## Files expected in the same folder\n\n- `test_bedrock_multimodal.ipynb`\n- `bedrock_harness.yaml`\n\nOptional local inputs (for understanding scenarios):\n\n- `./samples/demo_image.jpg`\n- `./samples/demo_video.mp4`\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# 0) Config (YAML) + Auth (env vars only)\n# =========================\nimport os\nfrom pathlib import Path\n\ntry:\n    import yaml\nexcept ImportError as e:\n    raise ImportError(\"Missing dependency: pyyaml. Run: pip install pyyaml\") from e\n\nCONFIG_ENV = \"BEDROCK_HARNESS_CONFIG\"\nCONFIG_PATH = os.environ.get(CONFIG_ENV, \"bedrock_harness.yaml\")\ncfg_path = Path(CONFIG_PATH).expanduser().resolve()\nCFG_BASE_DIR = cfg_path.parent\n\nif not cfg_path.exists():\n    raise FileNotFoundError(\n        f\"Missing config file: {cfg_path}\\n\"\n        \"Put 'bedrock_harness.yaml' next to this notebook, or set:\\n\"\n        f\"  export {CONFIG_ENV}=/path/to/bedrock_harness.yaml\"\n    )\n\ncfg = yaml.safe_load(cfg_path.read_text(encoding=\"utf-8\")) or {}\n\nauth_cfg = cfg.get(\"auth\", {}) or {}\nTOKEN_ENV = auth_cfg.get(\"token_env\", \"AWS_BEARER_TOKEN_BEDROCK\")\nREGION_ENV = auth_cfg.get(\"region_env\", \"AWS_REGION\")\nDEFAULT_REGION = auth_cfg.get(\"default_region\", \"us-east-1\")\n\ntoken = (os.environ.get(TOKEN_ENV) or \"\").strip()\nif not token or len(token) < 20:\n    raise RuntimeError(\n        f\"Missing env var {TOKEN_ENV}.\\n\\n\"\n        \"Export it before launching VS Code / Jupyter, e.g.:\\n\"\n        f\"  export {TOKEN_ENV}='ABSK...'\\n\"\n        f\"  export {REGION_ENV}='us-east-1'\"\n    )\n\nAWS_REGION = (os.environ.get(REGION_ENV) or DEFAULT_REGION).strip()\nos.environ[REGION_ENV] = AWS_REGION\n\nmm_cfg = cfg.get(\"multimodal\", {}) or {}\nscenario_names = sorted((mm_cfg.get(\"scenarios\", {}) or {}).keys())\ndefault_run = mm_cfg.get(\"default_run\", []) or []\n\nprint(\"Loaded config:\", str(cfg_path))\nprint(\"Region:\", AWS_REGION)\nprint(\"Token env:\", TOKEN_ENV, \"=\", \"set\" if bool(os.environ.get(TOKEN_ENV)) else \"missing\")\nprint(\"Multimodal scenarios:\", \", \".join(scenario_names) if scenario_names else \"(none)\")\nprint(\"Default multimodal run:\", default_run)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# 1) Multimodal harness (invoke + bearer token; config-driven)\n# =========================\nfrom __future__ import annotations\n\nimport base64\nimport json\nimport mimetypes\nimport re\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport requests\n\n\n_FULL_PLACEHOLDER = re.compile(r\"^\\{([A-Za-z_][A-Za-z0-9_]*)\\}$\")\n_ANY_PLACEHOLDER = re.compile(r\"\\{([A-Za-z_][A-Za-z0-9_]*)\\}\")\n\n_IMAGE_EXTS = {\".png\", \".jpg\", \".jpeg\", \".webp\", \".gif\", \".bmp\"}\n_VIDEO_EXTS = {\".mp4\", \".mov\", \".m4v\", \".avi\", \".webm\", \".mkv\"}\n\n\ndef _deep_get(data: Any, path: str) -> Any:\n    if not path:\n        return data\n    cur = data\n    for part in path.split(\".\"):\n        if isinstance(cur, dict):\n            cur = cur.get(part)\n        elif isinstance(cur, list) and part.isdigit():\n            idx = int(part)\n            if idx < 0 or idx >= len(cur):\n                return None\n            cur = cur[idx]\n        else:\n            return None\n    return cur\n\n\ndef _render_template(value: Any, ctx: Dict[str, Any]) -> Any:\n    if isinstance(value, dict):\n        return {k: _render_template(v, ctx) for k, v in value.items()}\n    if isinstance(value, list):\n        return [_render_template(v, ctx) for v in value]\n    if isinstance(value, str):\n        m = _FULL_PLACEHOLDER.match(value)\n        if m:\n            key = m.group(1)\n            if key not in ctx:\n                raise KeyError(f\"Missing template var: {key}\")\n            return ctx[key]\n\n        def repl(match: re.Match[str]) -> str:\n            key = match.group(1)\n            if key not in ctx:\n                raise KeyError(f\"Missing template var: {key}\")\n            return str(ctx[key])\n\n        return _ANY_PLACEHOLDER.sub(repl, value)\n\n    return value\n\n\nclass BedrockMultimodalHarness:\n    def __init__(self, cfg: Dict[str, Any], config_dir: Path):\n        self.cfg = cfg or {}\n        self.config_dir = config_dir\n\n        auth = self.cfg.get(\"auth\", {}) or {}\n        self.token_env = auth.get(\"token_env\", \"AWS_BEARER_TOKEN_BEDROCK\")\n        self.region_env = auth.get(\"region_env\", \"AWS_REGION\")\n        self.default_region = auth.get(\"default_region\", \"us-east-1\")\n\n        http = self.cfg.get(\"http\", {}) or {}\n        self.timeout_seconds = int(http.get(\"timeout_seconds\", 120))\n\n        endpoints = self.cfg.get(\"endpoints\", {}) or {}\n        self.runtime_base_tmpl = endpoints.get(\"bedrock_runtime_base\", \"https://bedrock-runtime.{region}.amazonaws.com\")\n\n        providers = self.cfg.get(\"providers\", {}) or {}\n        self.anthropic_version = ((providers.get(\"anthropic\", {}) or {}).get(\"anthropic_version\")) or \"bedrock-2023-05-31\"\n\n        self.models = self.cfg.get(\"models\", {}) or {}\n\n        self.mm = self.cfg.get(\"multimodal\", {}) or {}\n        self.scenarios = self.mm.get(\"scenarios\", {}) or {}\n        self.default_run = list(self.mm.get(\"default_run\", []) or [])\n        self.output_dir = self._resolve_path(self.mm.get(\"output_dir\", \"./outputs\"))\n\n        self._session = requests.Session()\n\n    # ---- env ----\n    def region(self) -> str:\n        return (os.environ.get(self.region_env) or self.default_region).strip()\n\n    def bearer_token(self) -> str:\n        tok = (os.environ.get(self.token_env) or \"\").strip()\n        if not tok or len(tok) < 20:\n            raise RuntimeError(f\"Missing env var {self.token_env}.\")\n        return tok\n\n    def runtime_base(self) -> str:\n        return self.runtime_base_tmpl.format(region=self.region()).rstrip(\"/\")\n\n    def headers_json(self) -> Dict[str, str]:\n        return {\n            \"Authorization\": f\"Bearer {self.bearer_token()}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    # ---- helpers ----\n    def _resolve_path(self, p: str) -> Path:\n        raw = Path(p).expanduser()\n        if raw.is_absolute():\n            return raw.resolve()\n        return (self.config_dir / raw).resolve()\n\n    def _resolve_model(self, model_ref: str) -> Dict[str, str]:\n        if model_ref not in self.models:\n            raise ValueError(f\"Unknown model_ref '{model_ref}'. Available: {sorted(self.models.keys())}\")\n        m = self.models[model_ref] or {}\n        provider = m.get(\"provider\")\n        model_id = m.get(\"model_id\")\n        if not provider or not model_id:\n            raise ValueError(f\"Invalid model config for '{model_ref}': provider/model_id required\")\n        return {\"provider\": provider, \"model_id\": model_id}\n\n    def _prepare_media(self, scenario: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n        media_cfg = scenario.get(\"media\", []) or []\n        media_list: List[Dict[str, Any]] = []\n        ctx: Dict[str, Any] = {}\n\n        for idx, item in enumerate(media_cfg, start=1):\n            name = item.get(\"name\") or f\"media_{idx}\"\n            mtype = item.get(\"type\", \"image\")\n            path_val = item.get(\"path\")\n            if not path_val:\n                raise ValueError(f\"Scenario media '{name}' missing path\")\n\n            path = self._resolve_path(path_val)\n            if not path.exists():\n                raise FileNotFoundError(\n                    f\"Media file not found for scenario item '{name}': {path}\"\n                )\n\n            media_type = item.get(\"media_type\")\n            if not media_type:\n                guessed, _ = mimetypes.guess_type(str(path))\n                media_type = guessed or (\"image/jpeg\" if mtype == \"image\" else \"video/mp4\")\n\n            fmt = item.get(\"format\")\n            if not fmt:\n                if \"/\" in media_type:\n                    fmt = media_type.split(\"/\", 1)[1].split(\";\", 1)[0]\n                else:\n                    fmt = path.suffix.lstrip(\".\")\n\n            b64 = base64.b64encode(path.read_bytes()).decode(\"ascii\")\n\n            media_list.append(\n                {\n                    \"name\": name,\n                    \"type\": mtype,\n                    \"path\": str(path),\n                    \"media_type\": media_type,\n                    \"format\": fmt,\n                    \"base64\": b64,\n                }\n            )\n\n            ctx[f\"{name}_path\"] = str(path)\n            ctx[f\"{name}_media_type\"] = media_type\n            ctx[f\"{name}_format\"] = fmt\n            ctx[f\"{name}_base64\"] = b64\n\n        return media_list, ctx\n\n    def _extract_first(self, data: Dict[str, Any], paths: List[str]) -> Tuple[Any, Optional[str]]:\n        for p in paths:\n            v = _deep_get(data, p)\n            if v is not None:\n                return v, p\n        return None, None\n\n    def _save_base64_to_file(self, b64_text: str, save_dir: str, file_name: str) -> Path:\n        out_dir = (self.output_dir / save_dir).resolve()\n        out_dir.mkdir(parents=True, exist_ok=True)\n        out_path = out_dir / file_name\n        out_path.write_bytes(base64.b64decode(b64_text))\n        return out_path\n\n    def _parse_response(self, scenario_name: str, response_cfg: Dict[str, Any], resp_json: Dict[str, Any]) -> Dict[str, Any]:\n        parsed: Dict[str, Any] = {\n            \"response_text\": None,\n            \"saved_files\": [],\n            \"job_id\": None,\n        }\n\n        rtype = (response_cfg or {}).get(\"type\", \"json\")\n\n        if rtype == \"anthropic_text\":\n            blocks = resp_json.get(\"content\", []) or []\n            parts = [b.get(\"text\", \"\") for b in blocks if isinstance(b, dict) and b.get(\"type\") == \"text\"]\n            parsed[\"response_text\"] = \"\".join(parts).strip()\n\n        elif rtype == \"text_path\":\n            path = (response_cfg or {}).get(\"path\", \"\")\n            v = _deep_get(resp_json, path)\n            parsed[\"response_text\"] = \"\" if v is None else str(v)\n\n        elif rtype == \"base64_images\":\n            path = (response_cfg or {}).get(\"path\", \"images\")\n            images = _deep_get(resp_json, path)\n            save_dir = (response_cfg or {}).get(\"save_dir\", scenario_name)\n            ext = ((response_cfg or {}).get(\"file_ext\", \"png\") or \"png\").lstrip(\".\")\n\n            if isinstance(images, list):\n                for i, b64_text in enumerate(images, start=1):\n                    if isinstance(b64_text, str) and b64_text.strip():\n                        file_name = f\"{scenario_name}_{i:02d}.{ext}\"\n                        out_path = self._save_base64_to_file(b64_text, save_dir, file_name)\n                        parsed[\"saved_files\"].append(str(out_path))\n\n        elif rtype == \"base64_video_or_job\":\n            save_dir = (response_cfg or {}).get(\"save_dir\", scenario_name)\n            ext = ((response_cfg or {}).get(\"file_ext\", \"mp4\") or \"mp4\").lstrip(\".\")\n            video_paths = (response_cfg or {}).get(\n                \"video_path_candidates\",\n                [\"video\", \"videoBase64\", \"output.video\", \"result.video\"],\n            )\n            job_paths = (response_cfg or {}).get(\n                \"job_id_candidates\",\n                [\"invocationArn\", \"jobArn\", \"jobId\", \"id\"],\n            )\n\n            video_blob, _ = self._extract_first(resp_json, video_paths)\n            if isinstance(video_blob, str) and len(video_blob) > 100:\n                file_name = f\"{scenario_name}.{ext}\"\n                out_path = self._save_base64_to_file(video_blob, save_dir, file_name)\n                parsed[\"saved_files\"].append(str(out_path))\n\n            job_id, _ = self._extract_first(resp_json, job_paths)\n            if job_id is not None:\n                parsed[\"job_id\"] = str(job_id)\n\n        return parsed\n\n    # ---- scenario runners ----\n    def _run_invoke_template(self, scenario_name: str, scenario: Dict[str, Any], model: Dict[str, str]) -> Dict[str, Any]:\n        template_vars = dict(scenario.get(\"template_vars\", {}) or {})\n        media_list, media_ctx = self._prepare_media(scenario)\n        template_vars.update(media_ctx)\n\n        req_tmpl = scenario.get(\"request_template\")\n        if req_tmpl is None:\n            raise ValueError(f\"Scenario '{scenario_name}' kind=invoke_template requires request_template\")\n\n        payload = _render_template(req_tmpl, template_vars)\n\n        endpoint_path = (scenario.get(\"endpoint_path\") or \"/model/{model_id}/invoke\").format(\n            model_id=model[\"model_id\"]\n        )\n        url = f\"{self.runtime_base()}{endpoint_path}\"\n        timeout = int(scenario.get(\"timeout_seconds\", self.timeout_seconds))\n\n        r = self._session.post(url, headers=self.headers_json(), data=json.dumps(payload), timeout=timeout)\n        if r.status_code != 200:\n            raise RuntimeError(f\"Invoke failed {r.status_code}: {r.text[:1200]}\")\n\n        resp_json = r.json()\n        response_cfg = scenario.get(\"response\", {}) or {}\n        parsed = self._parse_response(scenario_name, response_cfg, resp_json)\n\n        return {\n            \"endpoint_path\": endpoint_path,\n            \"media_count\": len(media_list),\n            \"response_json\": resp_json,\n            **parsed,\n        }\n\n    def _run_anthropic_messages(self, scenario_name: str, scenario: Dict[str, Any], model: Dict[str, str]) -> Dict[str, Any]:\n        if model[\"provider\"] != \"anthropic\":\n            raise ValueError(\n                f\"Scenario '{scenario_name}' kind=anthropic_messages requires an anthropic provider model_ref\"\n            )\n\n        media_list, _ = self._prepare_media(scenario)\n        user_prompt = str(scenario.get(\"user_prompt\", \"\") or \"\").strip()\n        system_prompt = str(scenario.get(\"system_prompt\", \"\") or \"\").strip()\n\n        content: List[Dict[str, Any]] = []\n        for m in media_list:\n            content.append(\n                {\n                    \"type\": m[\"type\"],\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": m[\"media_type\"],\n                        \"data\": m[\"base64\"],\n                    },\n                }\n            )\n\n        if user_prompt:\n            content.append({\"type\": \"text\", \"text\": user_prompt})\n\n        if not content:\n            raise ValueError(f\"Scenario '{scenario_name}' must provide media or user_prompt\")\n\n        payload: Dict[str, Any] = {\n            \"anthropic_version\": self.anthropic_version,\n            \"messages\": [{\"role\": \"user\", \"content\": content}],\n            \"max_tokens\": int(scenario.get(\"max_output_tokens\", 1024)),\n        }\n        if system_prompt:\n            payload[\"system\"] = system_prompt\n        if \"temperature\" in scenario and scenario.get(\"temperature\") is not None:\n            payload[\"temperature\"] = float(scenario.get(\"temperature\"))\n\n        endpoint_path = f\"/model/{model['model_id']}/invoke\"\n        url = f\"{self.runtime_base()}{endpoint_path}\"\n        timeout = int(scenario.get(\"timeout_seconds\", self.timeout_seconds))\n\n        r = self._session.post(url, headers=self.headers_json(), data=json.dumps(payload), timeout=timeout)\n        if r.status_code != 200:\n            raise RuntimeError(f\"Anthropic multimodal invoke failed {r.status_code}: {r.text[:1200]}\")\n\n        resp_json = r.json()\n        response_cfg = scenario.get(\"response\", {}) or {\"type\": \"anthropic_text\"}\n        parsed = self._parse_response(scenario_name, response_cfg, resp_json)\n\n        usage = resp_json.get(\"usage\")\n\n        return {\n            \"endpoint_path\": endpoint_path,\n            \"media_count\": len(media_list),\n            \"usage\": usage,\n            \"response_json\": resp_json,\n            **parsed,\n        }\n\n    def run_scenario(self, scenario_name: str) -> Dict[str, Any]:\n        if scenario_name not in self.scenarios:\n            raise ValueError(f\"Unknown scenario '{scenario_name}'. Available: {sorted(self.scenarios.keys())}\")\n\n        scenario = self.scenarios[scenario_name] or {}\n        kind = scenario.get(\"kind\", \"invoke_template\")\n        model_ref = scenario.get(\"model_ref\")\n        if not model_ref:\n            raise ValueError(f\"Scenario '{scenario_name}' missing model_ref\")\n\n        model = self._resolve_model(model_ref)\n\n        t0 = time.time()\n        base_record: Dict[str, Any] = {\n            \"scenario\": scenario_name,\n            \"kind\": kind,\n            \"model_ref\": model_ref,\n            \"provider\": model[\"provider\"],\n            \"model_id\": model[\"model_id\"],\n            \"latency_s\": None,\n            \"error\": None,\n            \"response_text\": None,\n            \"saved_files\": [],\n            \"job_id\": None,\n            \"usage\": None,\n            \"response_json\": None,\n            \"media_count\": 0,\n            \"endpoint_path\": None,\n        }\n\n        try:\n            if kind == \"invoke_template\":\n                out = self._run_invoke_template(scenario_name, scenario, model)\n            elif kind == \"anthropic_messages\":\n                out = self._run_anthropic_messages(scenario_name, scenario, model)\n            else:\n                raise ValueError(f\"Unsupported scenario kind: {kind}\")\n\n            base_record.update(out)\n            base_record[\"latency_s\"] = round(time.time() - t0, 3)\n\n        except Exception as e:\n            base_record[\"latency_s\"] = round(time.time() - t0, 3)\n            base_record[\"error\"] = f\"{type(e).__name__}: {e}\"\n\n        return base_record\n\n    def run_suite(self, scenario_names: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n        names = scenario_names or self.default_run or sorted(self.scenarios.keys())\n        return [self.run_scenario(name) for name in names]\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# 2) Display helpers\n# =========================\nfrom IPython.display import Markdown, display, Image, Video\nimport html\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\n\ndef _as_text(x: Any) -> str:\n    s = \"\" if x is None else str(x)\n    return html.escape(s)\n\n\ndef _is_image_file(p: Path) -> bool:\n    return p.suffix.lower() in {\".png\", \".jpg\", \".jpeg\", \".webp\", \".gif\", \".bmp\"}\n\n\ndef _is_video_file(p: Path) -> bool:\n    return p.suffix.lower() in {\".mp4\", \".mov\", \".m4v\", \".avi\", \".webm\", \".mkv\"}\n\n\ndef display_multimodal_results(records: List[Dict[str, Any]]):\n    for rec in records:\n        title = f\"{rec.get('scenario')} | {rec.get('kind')} | {rec.get('model_ref')}\"\n        meta_lines = [\n            f\"provider: {rec.get('provider')}\",\n            f\"model_id: {rec.get('model_id')}\",\n            f\"endpoint: {rec.get('endpoint_path')}\",\n            f\"latency_s: {rec.get('latency_s')}\",\n            f\"media_count: {rec.get('media_count')}\",\n        ]\n\n        if rec.get(\"job_id\"):\n            meta_lines.append(f\"job_id: {rec.get('job_id')}\")\n        if rec.get(\"usage\"):\n            meta_lines.append(f\"usage: {rec.get('usage')}\")\n\n        if rec.get(\"error\"):\n            meta_lines.append(f\"error: {rec.get('error')}\")\n\n        body = \"\\n\".join([f\"- {line}\" for line in meta_lines])\n        display(Markdown(f\"### {title}\\n{body}\"))\n\n        if rec.get(\"response_text\"):\n            txt = rec.get(\"response_text\")\n            display(Markdown(\"**Response text**\"))\n            display(Markdown(f\"```text\\n{txt}\\n```\"))\n\n        saved_files = rec.get(\"saved_files\") or []\n        if saved_files:\n            display(Markdown(\"**Saved artifacts**\"))\n            for fp in saved_files:\n                p = Path(fp)\n                display(Markdown(f\"- `{p}`\"))\n                if p.exists() and _is_image_file(p):\n                    display(Image(filename=str(p)))\n                elif p.exists() and _is_video_file(p):\n                    display(Video(filename=str(p), embed=True))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# 3) Run default multimodal suite from YAML\n# =========================\nmm_harness = BedrockMultimodalHarness(cfg, config_dir=CFG_BASE_DIR)\n\nresults = mm_harness.run_suite()\ndisplay_multimodal_results(results)\n\nresults\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}