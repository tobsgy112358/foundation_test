{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bedrock local test notebook (invoke + bearer token)\n",
        "\n",
        "This notebook runs **locally** (VS Code / Jupyter) and calls **Amazon Bedrock Runtime** via HTTPS using a team-owned **Bedrock long-term API key** (bearer token).\n",
        "\n",
        "## One-time setup (recommended)\n",
        "\n",
        "1. Export these env vars **before you launch VS Code** (so the Jupyter kernel inherits them):\n",
        "\n",
        "```bash\n",
        "export AWS_BEARER_TOKEN_BEDROCK=\"ABSK...\"\n",
        "export AWS_REGION=\"us-east-1\"   # optional (or set a different enabled region)\n",
        "```\n",
        "\n",
        "> If you previously opened VS Code, fully quit it (Cmd+Q) and re-open it **after** exporting the env vars.\n",
        "\n",
        "2. Put these two files in the same folder:\n",
        "\n",
        "- `test_bedrock.ipynb`\n",
        "- `bedrock_harness.yaml`\n",
        "\n",
        "3. Install deps in your venv once:\n",
        "\n",
        "```bash\n",
        "pip install requests pyyaml\n",
        "```\n",
        "\n",
        "## What you get\n",
        "\n",
        "- A single helper: `get_completion(system, user, model=\"preset_name\")`\n",
        "- Preset-based fallback across providers (Claude-first, with explicit Nova / gpt-oss fallbacks)\n",
        "- A side-by-side A/B compare display that shows:\n",
        "  - preset, provider, model_ref, model_id\n",
        "  - latency + token usage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 0) Config (YAML) + Auth (env vars only)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    import yaml\n",
        "except ImportError as e:\n",
        "    raise ImportError(\"Missing dependency: pyyaml. Run: pip install pyyaml\") from e\n",
        "\n",
        "CONFIG_ENV = \"BEDROCK_HARNESS_CONFIG\"\n",
        "CONFIG_PATH = os.environ.get(CONFIG_ENV, \"bedrock_harness.yaml\")\n",
        "cfg_path = Path(CONFIG_PATH).expanduser().resolve()\n",
        "\n",
        "if not cfg_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing config file: {cfg_path}\\n\"\n",
        "        \"Put 'bedrock_harness.yaml' next to this notebook, or set:\\n\"\n",
        "        f\"  export {CONFIG_ENV}=/path/to/bedrock_harness.yaml\"\n",
        "    )\n",
        "\n",
        "cfg = yaml.safe_load(cfg_path.read_text(encoding=\"utf-8\")) or {}\n",
        "\n",
        "auth_cfg = cfg.get(\"auth\", {}) or {}\n",
        "TOKEN_ENV = auth_cfg.get(\"token_env\", \"AWS_BEARER_TOKEN_BEDROCK\")\n",
        "REGION_ENV = auth_cfg.get(\"region_env\", \"AWS_REGION\")\n",
        "DEFAULT_REGION = auth_cfg.get(\"default_region\", \"us-east-1\")\n",
        "\n",
        "# --- Token must be provided via env var (no prompts, no YAML secrets) ---\n",
        "token = (os.environ.get(TOKEN_ENV) or \"\").strip()\n",
        "if not token or len(token) < 20:\n",
        "    raise RuntimeError(\n",
        "        f\"Missing env var {TOKEN_ENV}.\\n\\n\"\n",
        "        \"Export it *before launching VS Code / Jupyter*, e.g.:\\n\"\n",
        "        f\"  export {TOKEN_ENV}='ABSK...'\\n\"\n",
        "        f\"  export {REGION_ENV}='us-east-1'\\n\\n\"\n",
        "        \"Then fully quit VS Code (Cmd+Q) and re-open it.\"\n",
        "    )\n",
        "\n",
        "# Region: env var overrides YAML default_region\n",
        "AWS_REGION = (os.environ.get(REGION_ENV) or DEFAULT_REGION).strip()\n",
        "os.environ[REGION_ENV] = AWS_REGION\n",
        "\n",
        "defaults_cfg = cfg.get(\"defaults\", {}) or {}\n",
        "DEFAULT_PRESET = defaults_cfg.get(\"preset\", \"analysis_max\")\n",
        "DEBUG_FALLBACK_DEFAULT = bool(defaults_cfg.get(\"debug_fallback\", True))\n",
        "\n",
        "print(\"✅ Loaded config:\", str(cfg_path))\n",
        "print(\"   Region:\", AWS_REGION)\n",
        "print(\"   Token env:\", TOKEN_ENV, \"=\", \"set\" if bool(os.environ.get(TOKEN_ENV)) else \"missing\")\n",
        "print(\"   Presets:\", \", \".join(sorted((cfg.get(\"presets\", {}) or {}).keys())))\n",
        "print(\"   Default preset:\", DEFAULT_PRESET)\n",
        "print(\"   Debug fallback:\", DEBUG_FALLBACK_DEFAULT)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1) Harness (invoke + bearer token; config-driven)\n",
        "# =========================\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ModelSpec:\n",
        "    preset: str\n",
        "    model_ref: str\n",
        "    provider: str          # \"anthropic\" | \"nova\" | \"openai_compat\"\n",
        "    model_id: str          # Bedrock modelId / inference profile id\n",
        "    temperature: Optional[float]\n",
        "    max_output_tokens: int\n",
        "\n",
        "    # Anthropic-only knobs\n",
        "    anthropic_thinking_enabled: bool = False\n",
        "    anthropic_thinking_budget_tokens: Optional[int] = None\n",
        "\n",
        "\n",
        "def _normalize_usage(provider: str, resp_json: Dict[str, Any]) -> Dict[str, Optional[int]]:\n",
        "    \"\"\"Normalize usage fields into {input_tokens, output_tokens, total_tokens}.\"\"\"\n",
        "    u = (resp_json or {}).get(\"usage\") or {}\n",
        "    inp = out = total = None\n",
        "\n",
        "    if provider == \"anthropic\":\n",
        "        inp = u.get(\"input_tokens\")\n",
        "        out = u.get(\"output_tokens\")\n",
        "        total = u.get(\"total_tokens\")\n",
        "\n",
        "    elif provider == \"nova\":\n",
        "        # Nova tends to use camelCase in examples; handle both.\n",
        "        inp = u.get(\"inputTokens\", u.get(\"input_tokens\"))\n",
        "        out = u.get(\"outputTokens\", u.get(\"output_tokens\"))\n",
        "        total = u.get(\"totalTokens\", u.get(\"total_tokens\"))\n",
        "\n",
        "    elif provider == \"openai_compat\":\n",
        "        inp = u.get(\"prompt_tokens\", u.get(\"input_tokens\"))\n",
        "        out = u.get(\"completion_tokens\", u.get(\"output_tokens\"))\n",
        "        total = u.get(\"total_tokens\")\n",
        "\n",
        "    # Best-effort total\n",
        "    if total is None and (inp is not None or out is not None):\n",
        "        total = (inp or 0) + (out or 0)\n",
        "\n",
        "    return {\n",
        "        \"input_tokens\": int(inp) if inp is not None else None,\n",
        "        \"output_tokens\": int(out) if out is not None else None,\n",
        "        \"total_tokens\": int(total) if total is not None else None,\n",
        "    }\n",
        "\n",
        "\n",
        "class BedrockHarness:\n",
        "    def __init__(self, cfg: Dict[str, Any]):\n",
        "        self.cfg = cfg or {}\n",
        "\n",
        "        auth = self.cfg.get(\"auth\", {}) or {}\n",
        "        self.token_env = auth.get(\"token_env\", \"AWS_BEARER_TOKEN_BEDROCK\")\n",
        "        self.region_env = auth.get(\"region_env\", \"AWS_REGION\")\n",
        "        self.default_region = auth.get(\"default_region\", \"us-east-1\")\n",
        "\n",
        "        http = self.cfg.get(\"http\", {}) or {}\n",
        "        self.timeout_seconds = int(http.get(\"timeout_seconds\", 60))\n",
        "\n",
        "        defaults = self.cfg.get(\"defaults\", {}) or {}\n",
        "        self.default_preset = defaults.get(\"preset\", \"analysis_max\")\n",
        "        self.debug_fallback_default = bool(defaults.get(\"debug_fallback\", True))\n",
        "\n",
        "        endpoints = self.cfg.get(\"endpoints\", {}) or {}\n",
        "        self.runtime_base_tmpl = endpoints.get(\"bedrock_runtime_base\", \"https://bedrock-runtime.{region}.amazonaws.com\")\n",
        "\n",
        "        providers = self.cfg.get(\"providers\", {}) or {}\n",
        "        self.anthropic_version = ((providers.get(\"anthropic\", {}) or {}).get(\"anthropic_version\")) or \"bedrock-2023-05-31\"\n",
        "\n",
        "        openai_cfg = providers.get(\"openai_compat\", {}) or {}\n",
        "        self.openai_chat_path = openai_cfg.get(\"chat_completions_path\", \"/openai/v1/chat/completions\")\n",
        "        self.openai_system_role = openai_cfg.get(\"system_role\", \"developer\")\n",
        "        self.openai_max_tokens_param = openai_cfg.get(\"max_tokens_param\", \"max_completion_tokens\")\n",
        "\n",
        "        self._models = self.cfg.get(\"models\", {}) or {}\n",
        "        self._presets = self.cfg.get(\"presets\", {}) or {}\n",
        "\n",
        "        # Reuse connections\n",
        "        self._session = requests.Session()\n",
        "\n",
        "    # ---- env ----\n",
        "    def region(self) -> str:\n",
        "        return (os.environ.get(self.region_env) or self.default_region).strip()\n",
        "\n",
        "    def bearer_token(self) -> str:\n",
        "        tok = (os.environ.get(self.token_env) or \"\").strip()\n",
        "        if not tok or len(tok) < 20:\n",
        "            raise RuntimeError(f\"Missing env var {self.token_env}.\")\n",
        "        return tok\n",
        "\n",
        "    def runtime_base(self) -> str:\n",
        "        return self.runtime_base_tmpl.format(region=self.region()).rstrip(\"/\")\n",
        "\n",
        "    def headers_json(self) -> Dict[str, str]:\n",
        "        return {\"Authorization\": f\"Bearer {self.bearer_token()}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "    # ---- config -> ModelSpec ----\n",
        "    def build_preset_specs(self, preset: str) -> List[ModelSpec]:\n",
        "        if preset not in self._presets:\n",
        "            raise ValueError(f\"Unknown preset '{preset}'. Available: {sorted(self._presets.keys())}\")\n",
        "\n",
        "        out: List[ModelSpec] = []\n",
        "        for item in self._presets[preset]:\n",
        "            ref = item[\"model_ref\"]\n",
        "            if ref not in self._models:\n",
        "                raise ValueError(f\"Preset '{preset}' references unknown model_ref '{ref}'.\")\n",
        "\n",
        "            m = self._models[ref]\n",
        "            provider = m[\"provider\"]\n",
        "            model_id = m[\"model_id\"]\n",
        "\n",
        "            temperature = item.get(\"temperature\", None)\n",
        "            max_output_tokens = int(item[\"max_output_tokens\"])\n",
        "\n",
        "            thinking_enabled = False\n",
        "            thinking_budget = None\n",
        "            if provider == \"anthropic\":\n",
        "                anth = item.get(\"anthropic\", {}) or {}\n",
        "                thinking = (anth.get(\"thinking\", {}) or {})\n",
        "                thinking_enabled = bool(thinking.get(\"enabled\", False))\n",
        "                if \"budget_tokens\" in thinking:\n",
        "                    thinking_budget = int(thinking[\"budget_tokens\"])\n",
        "\n",
        "                # --- Guardrails for Claude extended thinking ---\n",
        "                # 1) When thinking is enabled, temperature must be 1.\n",
        "                if thinking_enabled:\n",
        "                    temperature = 1\n",
        "\n",
        "                # 2) max_output_tokens must be > thinking.budget_tokens.\n",
        "                if thinking_enabled and thinking_budget is not None and thinking_budget >= max_output_tokens:\n",
        "                    thinking_budget = max_output_tokens - 1\n",
        "\n",
        "            out.append(ModelSpec(\n",
        "                preset=preset,\n",
        "                model_ref=ref,\n",
        "                provider=provider,\n",
        "                model_id=model_id,\n",
        "                temperature=None if temperature is None else float(temperature),\n",
        "                max_output_tokens=max_output_tokens,\n",
        "                anthropic_thinking_enabled=thinking_enabled,\n",
        "                anthropic_thinking_budget_tokens=thinking_budget,\n",
        "            ))\n",
        "\n",
        "        return out\n",
        "\n",
        "    # ---- provider adapters ----\n",
        "    def _invoke_anthropic(self, spec: ModelSpec, system_prompt: Optional[str], user_prompt: str) -> Tuple[str, Dict[str, Any], Dict[str, Optional[int]]]:\n",
        "        url = f\"{self.runtime_base()}/model/{spec.model_id}/invoke\"\n",
        "\n",
        "        payload: Dict[str, Any] = {\n",
        "            \"anthropic_version\": self.anthropic_version,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_prompt}]}],\n",
        "            \"max_tokens\": int(spec.max_output_tokens),\n",
        "        }\n",
        "        if system_prompt:\n",
        "            payload[\"system\"] = system_prompt\n",
        "        if spec.temperature is not None:\n",
        "            payload[\"temperature\"] = float(spec.temperature)\n",
        "\n",
        "        if spec.anthropic_thinking_enabled:\n",
        "            thinking: Dict[str, Any] = {\"type\": \"enabled\"}\n",
        "            if spec.anthropic_thinking_budget_tokens is not None:\n",
        "                thinking[\"budget_tokens\"] = int(spec.anthropic_thinking_budget_tokens)\n",
        "            payload[\"thinking\"] = thinking\n",
        "\n",
        "        r = self._session.post(url, headers=self.headers_json(), data=json.dumps(payload), timeout=self.timeout_seconds)\n",
        "        if r.status_code != 200:\n",
        "            raise RuntimeError(f\"Anthropic invoke failed {r.status_code}: {r.text[:1200]}\")\n",
        "\n",
        "        resp = r.json()\n",
        "        parts = [c.get(\"text\", \"\") for c in resp.get(\"content\", []) if isinstance(c, dict) and c.get(\"type\") == \"text\"]\n",
        "        text = \"\".join(parts).strip()\n",
        "        usage_norm = _normalize_usage(\"anthropic\", resp)\n",
        "        return text, resp, usage_norm\n",
        "\n",
        "    def _invoke_nova(self, spec: ModelSpec, system_prompt: Optional[str], user_prompt: str) -> Tuple[str, Dict[str, Any], Dict[str, Optional[int]]]:\n",
        "        url = f\"{self.runtime_base()}/model/{spec.model_id}/invoke\"\n",
        "\n",
        "        payload: Dict[str, Any] = {\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": user_prompt}]}],\n",
        "            \"inferenceConfig\": {\"maxTokens\": int(spec.max_output_tokens)},\n",
        "        }\n",
        "        if spec.temperature is not None:\n",
        "            payload[\"inferenceConfig\"][\"temperature\"] = float(spec.temperature)\n",
        "        if system_prompt:\n",
        "            payload[\"system\"] = [{\"text\": system_prompt}]\n",
        "\n",
        "        r = self._session.post(url, headers=self.headers_json(), data=json.dumps(payload), timeout=self.timeout_seconds)\n",
        "        if r.status_code != 200:\n",
        "            raise RuntimeError(f\"Nova invoke failed {r.status_code}: {r.text[:1200]}\")\n",
        "\n",
        "        resp = r.json()\n",
        "        content = (((resp.get(\"output\") or {}).get(\"message\") or {}).get(\"content\")) or []\n",
        "        parts = [c.get(\"text\", \"\") for c in content if isinstance(c, dict) and \"text\" in c]\n",
        "        text = \"\".join(parts).strip()\n",
        "        usage_norm = _normalize_usage(\"nova\", resp)\n",
        "        return text, resp, usage_norm\n",
        "\n",
        "    def _openai_compat_chat(self, spec: ModelSpec, system_prompt: Optional[str], user_prompt: str) -> Tuple[str, Dict[str, Any], Dict[str, Optional[int]]]:\n",
        "        url = f\"{self.runtime_base()}{self.openai_chat_path}\"\n",
        "\n",
        "        messages: List[Dict[str, Any]] = []\n",
        "        if system_prompt:\n",
        "            messages.append({\"role\": self.openai_system_role, \"content\": system_prompt})\n",
        "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "        payload: Dict[str, Any] = {\n",
        "            \"model\": spec.model_id,\n",
        "            \"messages\": messages,\n",
        "            \"stream\": False,\n",
        "        }\n",
        "        if spec.temperature is not None:\n",
        "            payload[\"temperature\"] = float(spec.temperature)\n",
        "        payload[self.openai_max_tokens_param] = int(spec.max_output_tokens)\n",
        "\n",
        "        r = self._session.post(url, headers=self.headers_json(), data=json.dumps(payload), timeout=self.timeout_seconds)\n",
        "        if r.status_code != 200:\n",
        "            raise RuntimeError(f\"OpenAI-compat chat failed {r.status_code}: {r.text[:1200]}\")\n",
        "\n",
        "        resp = r.json()\n",
        "        text = (resp.get(\"choices\", [{}])[0].get(\"message\", {}) or {}).get(\"content\", \"\") or \"\"\n",
        "        usage_norm = _normalize_usage(\"openai_compat\", resp)\n",
        "        return text, resp, usage_norm\n",
        "\n",
        "    # ---- public API ----\n",
        "    def get_completion(\n",
        "        self,\n",
        "        system_prompt: Optional[str],\n",
        "        user_prompt: str,\n",
        "        model: Optional[str] = None,\n",
        "        *,\n",
        "        return_record: bool = False,\n",
        "        label: str = \"\",\n",
        "        debug_fallback: Optional[bool] = None,\n",
        "    ) -> Union[str, Dict[str, Any]]:\n",
        "        preset = model or self.default_preset\n",
        "        if debug_fallback is None:\n",
        "            debug_fallback = self.debug_fallback_default\n",
        "\n",
        "        candidates = self.build_preset_specs(preset)\n",
        "\n",
        "        last_err: Optional[Exception] = None\n",
        "        t0 = time.time()\n",
        "\n",
        "        for idx, spec in enumerate(candidates, start=1):\n",
        "            try:\n",
        "                if spec.provider == \"anthropic\":\n",
        "                    text, raw, usage = self._invoke_anthropic(spec, system_prompt, user_prompt)\n",
        "                elif spec.provider == \"nova\":\n",
        "                    text, raw, usage = self._invoke_nova(spec, system_prompt, user_prompt)\n",
        "                elif spec.provider == \"openai_compat\":\n",
        "                    text, raw, usage = self._openai_compat_chat(spec, system_prompt, user_prompt)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown provider: {spec.provider}\")\n",
        "\n",
        "                latency = time.time() - t0\n",
        "\n",
        "                if return_record:\n",
        "                    return {\n",
        "                        \"label\": label or preset,\n",
        "                        \"preset\": preset,\n",
        "                        \"picked_index\": idx,\n",
        "                        \"model_ref\": spec.model_ref,\n",
        "                        \"provider\": spec.provider,\n",
        "                        \"model_id\": spec.model_id,\n",
        "                        \"temperature\": spec.temperature,\n",
        "                        \"max_output_tokens\": spec.max_output_tokens,\n",
        "                        \"anthropic_thinking_enabled\": spec.anthropic_thinking_enabled,\n",
        "                        \"anthropic_thinking_budget_tokens\": spec.anthropic_thinking_budget_tokens,\n",
        "                        \"latency_s\": round(latency, 3),\n",
        "                        \"usage\": usage,\n",
        "                        \"usage_raw\": raw.get(\"usage\"),\n",
        "                        \"system_prompt\": system_prompt or \"\",\n",
        "                        \"user_prompt\": user_prompt,\n",
        "                        \"response\": text,\n",
        "                    }\n",
        "                return text\n",
        "\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "                if debug_fallback:\n",
        "                    print(f\"[fallback] preset={preset} model_ref={spec.model_ref} provider={spec.provider} failed: {type(e).__name__}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if return_record:\n",
        "            return {\n",
        "                \"label\": label or preset,\n",
        "                \"preset\": preset,\n",
        "                \"picked_index\": None,\n",
        "                \"model_ref\": None,\n",
        "                \"provider\": None,\n",
        "                \"model_id\": None,\n",
        "                \"temperature\": None,\n",
        "                \"max_output_tokens\": None,\n",
        "                \"anthropic_thinking_enabled\": None,\n",
        "                \"anthropic_thinking_budget_tokens\": None,\n",
        "                \"latency_s\": None,\n",
        "                \"usage\": None,\n",
        "                \"usage_raw\": None,\n",
        "                \"system_prompt\": system_prompt or \"\",\n",
        "                \"user_prompt\": user_prompt,\n",
        "                \"response\": f\"An error occurred: {last_err}\",\n",
        "            }\n",
        "        return f\"An error occurred: {last_err}\"\n",
        "\n",
        "\n",
        "# Instantiate once\n",
        "h = BedrockHarness(cfg)\n",
        "\n",
        "# Notebook-friendly alias (same signature as your original notebook)\n",
        "def get_completion(system_prompt, user_prompt, model=None, return_record: bool = False, label: str = \"\", debug_fallback: Optional[bool] = None):\n",
        "    return h.get_completion(system_prompt, user_prompt, model, return_record=return_record, label=label, debug_fallback=debug_fallback)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 2) Display helper (side-by-side compare + token usage)\n",
        "# =========================\n",
        "from IPython.display import Markdown, display\n",
        "import html\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "\n",
        "def display_responses(*records: Dict[str, Any], title: Optional[str] = None, max_col_width_px: int = 520):\n",
        "    \"\"\"Side-by-side display for harness records.\"\"\"\n",
        "\n",
        "    cols = list(records[0]) if (len(records) == 1 and isinstance(records[0], (list, tuple))) else list(records)\n",
        "\n",
        "    def esc(x: Any) -> str:\n",
        "        s = \"\" if x is None else str(x)\n",
        "        return html.escape(s).replace(\"\\n\", \"<br/>\")\n",
        "\n",
        "    def fmt_tokens(rec: Dict[str, Any]) -> str:\n",
        "        u = rec.get(\"usage\") or {}\n",
        "        if not isinstance(u, dict) or not u:\n",
        "            return \"\"\n",
        "        inp = u.get(\"input_tokens\")\n",
        "        out = u.get(\"output_tokens\")\n",
        "        tot = u.get(\"total_tokens\")\n",
        "        parts = []\n",
        "        if inp is not None:\n",
        "            parts.append(f\"in:{inp}\")\n",
        "        if out is not None:\n",
        "            parts.append(f\"out:{out}\")\n",
        "        if tot is not None:\n",
        "            parts.append(f\"total:{tot}\")\n",
        "        return \"tokens: \" + \" · \".join(parts) if parts else \"\"\n",
        "\n",
        "    def header_block(rec: Dict[str, Any]) -> str:\n",
        "        preset = rec.get(\"preset\")\n",
        "        provider = rec.get(\"provider\")\n",
        "        model_ref = rec.get(\"model_ref\")\n",
        "        model_id = rec.get(\"model_id\")\n",
        "        picked = rec.get(\"picked_index\")\n",
        "        lat = rec.get(\"latency_s\")\n",
        "        temp = rec.get(\"temperature\")\n",
        "        max_out = rec.get(\"max_output_tokens\")\n",
        "\n",
        "        thinking_on = rec.get(\"anthropic_thinking_enabled\")\n",
        "        thinking_budget = rec.get(\"anthropic_thinking_budget_tokens\")\n",
        "\n",
        "        meta_lines = []\n",
        "        if preset:\n",
        "            meta_lines.append(f\"preset: {preset}\")\n",
        "        if provider:\n",
        "            meta_lines.append(f\"provider: {provider}\")\n",
        "        if model_ref:\n",
        "            meta_lines.append(f\"model_ref: {model_ref}\")\n",
        "        if model_id:\n",
        "            meta_lines.append(f\"model_id: {model_id}\")\n",
        "        if picked is not None:\n",
        "            meta_lines.append(f\"picked: #{picked}\")\n",
        "        if lat is not None:\n",
        "            meta_lines.append(f\"latency: {lat}s\")\n",
        "        if max_out is not None:\n",
        "            meta_lines.append(f\"max_out: {max_out}\")\n",
        "        if temp is not None:\n",
        "            meta_lines.append(f\"temp: {temp}\")\n",
        "\n",
        "        if thinking_on:\n",
        "            meta_lines.append(\"thinking: on\")\n",
        "            if thinking_budget is not None:\n",
        "                meta_lines.append(f\"budget: {thinking_budget}\")\n",
        "        elif thinking_on is False and provider == \"anthropic\":\n",
        "            meta_lines.append(\"thinking: off\")\n",
        "\n",
        "        tok_line = fmt_tokens(rec)\n",
        "        if tok_line:\n",
        "            meta_lines.append(tok_line)\n",
        "\n",
        "        return \"<br/>\".join(esc(x) for x in meta_lines)\n",
        "\n",
        "    table = []\n",
        "    table.append(\"<div style='overflow-x:auto; padding:6px 2px;'>\")\n",
        "    table.append(\"<table style='border-collapse:collapse; width:max-content; min-width:100%;'>\")\n",
        "\n",
        "    if title:\n",
        "        table.append(\n",
        "            f\"<tr><th colspan='{len(cols)}' \"\n",
        "            \"style='text-align:left; font-size:16px; padding:10px; border:1px solid #ddd; background:#fafafa;'>\"\n",
        "            f\"{esc(title)}</th></tr>\"\n",
        "        )\n",
        "\n",
        "    # Headers\n",
        "    table.append(\"<tr>\")\n",
        "    for i, rec in enumerate(cols):\n",
        "        label = rec.get(\"label\") or f\"Variant {i+1}\"\n",
        "        meta = header_block(rec)\n",
        "        table.append(\n",
        "            \"<th style='text-align:left; vertical-align:top; border:1px solid #ddd; padding:10px; \"\n",
        "            \"background:#f7f7f7; min-width:320px; \"\n",
        "            f\"max-width:{max_col_width_px}px;'>\"\n",
        "            f\"{esc(label)}<br/><span style='font-weight:normal;color:#666'>{meta}</span></th>\"\n",
        "        )\n",
        "    table.append(\"</tr>\")\n",
        "\n",
        "    # Prompts\n",
        "    table.append(\"<tr>\")\n",
        "    for rec in cols:\n",
        "        sys = rec.get(\"system_prompt\", \"\")\n",
        "        usr = rec.get(\"user_prompt\", \"\")\n",
        "        table.append(\n",
        "            \"<td style='vertical-align:top; border:1px solid #ddd; padding:10px; \"\n",
        "            f\"max-width:{max_col_width_px}px;'>\"\n",
        "            f\"<div style='margin-bottom:10px;'><b>System</b><br/>{esc(sys)}</div>\"\n",
        "            f\"<div><b>User</b><br/>{esc(usr)}</div>\"\n",
        "            \"</td>\"\n",
        "        )\n",
        "    table.append(\"</tr>\")\n",
        "\n",
        "    # Responses\n",
        "    table.append(\"<tr>\")\n",
        "    for rec in cols:\n",
        "        resp = rec.get(\"response\", \"\")\n",
        "        table.append(\n",
        "            \"<td style='vertical-align:top; border:1px solid #ddd; padding:10px; \"\n",
        "            f\"max-width:{max_col_width_px}px;'>\"\n",
        "            \"<div><b>Response</b></div>\"\n",
        "            f\"<div style='margin-top:6px; line-height:1.35;'>{esc(resp)}</div>\"\n",
        "            \"</td>\"\n",
        "        )\n",
        "    table.append(\"</tr>\")\n",
        "\n",
        "    table.append(\"</table></div>\")\n",
        "    display(Markdown(\"\".join(table)))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3) Quick A/B test (analysis_max vs cheap_fast)\n",
        "# =========================\n",
        "SYSTEM = \"You are an AI engineering expert. Explain with metaphors and examples.\"\n",
        "USER = \"What's the difference of RAG, MCP, and SKILL? Explain it in Chinese.\"\n",
        "\n",
        "resp_max = get_completion(\n",
        "    SYSTEM,\n",
        "    USER,\n",
        "    model=\"analysis_max\",\n",
        "    return_record=True,\n",
        "    label=\"analysis_max\",\n",
        ")\n",
        "\n",
        "resp_fast = get_completion(\n",
        "    SYSTEM,\n",
        "    USER,\n",
        "    model=\"cheap_fast\",\n",
        "    return_record=True,\n",
        "    label=\"cheap_fast\",\n",
        ")\n",
        "\n",
        "display_responses(resp_max, resp_fast, title=\"Bedrock A/B: analysis_max vs cheap_fast\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.12 (bedrock-venv)",
      "language": "python",
      "name": "bedrock-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}